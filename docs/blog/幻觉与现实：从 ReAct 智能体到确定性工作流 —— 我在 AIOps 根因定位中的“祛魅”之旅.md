# 幻觉与现实：从 ReAct 智能体到确定性工作流 —— 我在 AIOps 根因定位中的“祛魅”之旅

> ​**摘要**​：本文复盘了一次尝试构建“运维版 Cursor”的失败经历。实验证明，即便通过预处理提供了高密度的运维数据，ReAct 模式依然无法带来正向收益。基于第一性原理，本文揭示了其根本原因不在于 prompt 或数据清洗，而在于​**大模型训练集在“运维排障轨迹”上的巨大空缺**​。因此，AIOps 的正确架构应当是：LLM 不做自主决策的驾驶员，只做固定 SOP 流程中的高智商节点。

---

## 引言：寻找运维领域的 "Cursor"

在大模型爆发的初期，我和许多开发者一样，怀揣着一个性感的构想：**能不能利用 ReAct（Reasoning and Acting）模式，打造一个运维领域的 Autonomous Agent？**

我的愿景很清晰：构建一个类似 Cursor 的智能助手，赋予它查询日志、调取监控指标（Metrics）、读取元数据（Metadata）的工具。期望它能像一位经验丰富的 SRE，面对报警，自主思考，自主调用工具，层层下钻，最终定位根因。

然而，这场从理想通往现实的实验，最终迫使我重写了整个系统架构。

## 一、 实验与困境：被“硬编码”的智能

在第一阶段，我投入了大量精力设计 Tool use 和优化 Prompt。但在实际运行中，ReAct 模式的效果令人失望。

我一度认为问题出在输入数据的质量上。于是，我通过精心的数据工程，将原始低密度的日志转化为**高密度、高逻辑自洽的聚合数据**喂给 LLM。我原本期望，数据质量的提升能激活 LLM 的推理能力，让它像分析代码一样分析运维数据。

但实验结果给了我当头一棒：

即便面对高质量的数据，Agent 依然无法展现出超越原有 SOP（标准作业程序）的能力。在面对特殊案例时，我不得不反复调试 Prompt，甚至把排障步骤隐式地写死在 Prompt 里才勉强跑通。

最终我发现：**这不就是用自然语言把固定代码重写了一遍吗？** 这种 Agent 不仅没有带来比固定流程更高的收益，反而引入了极大的不确定性和耗时。

## 二、 第一性原理反思：为什么“高质量数据”救不了 ReAct？

为什么代码场景（Cursor/Copilot）能成，而运维场景即便清洗了数据也跑不通？经过深度调研和反思，我找到了导致失败的​**终极变量**​。

### 1. 训练集的本质性缺失：不仅仅是数据，是“轨迹”

我们往往认为运维难是因为数据脏（日志是非结构化的）。但我的实验证明，即使把数据洗干净了，LLM 依然不懂怎么排障。

**核心原因在于：大模型厂商（OpenAI/Anthropic）手里根本没有运维场景的“思维链数据集”。**

* **代码世界（Github）：** 拥有万亿级的 Token。更重要的是，Github 上的 Issue 讨论、Pull Request 的修改记录、StackOverflow 的问答，构成了完整的 **Problem \$\\to\$ Context \$\\to\$ Reasoning \$\\to\$ Fix** 的闭环。LLM 见过无数次“错误是如何被纠正的”。
* **运维世界（黑盒）：** 极度封闭。
  * **没有哪家互联网大厂会把自己生产环境的故障排查全过程（Context + Action + Result）开源出来。**
  * 这些包含着 SRE 宝贵经验的“排障思维链”和“操作轨迹”，全部躺在公司内部的保密文档或工程师的大脑里，从未进入过 GPT 的预训练语料库。

### 2. 只有“状态”，没有“逻辑”

因此，目前的 LLM 在运维领域是“先天营养不良”的。它可能见过 Linux 手册（静态知识），但从未见过真实的、复杂的排障动态过程（过程知识）。

这就是为什么即便我喂给它高密度的数据，它依然只能做简单的模式匹配，而无法像真正的 SRE 那样进行复杂的假设验证和逻辑推演。它的“智商”上限，已经被训练数据锁死在了通用常识水平，远低于企业内部沉淀多年的 SOP。

## 三、 特例辨析：K8s Native Agent 为什么能成？

调研中，我发现市场上唯一的“成功者”是类似 K8sGPT、HolmesGPT 这类产品。这似乎反驳了我的观点？

**不，这恰恰验证了我的观点。**

K8s Agent 的成功建立在 Kubernetes 极度标准化的世界观之上：

* Pod 就是 Pod，Service 就是 Service。
* 状态机是有限且封闭的（Pending, Running, CrashLoopBackOff）。

K8s Agent 并不需要“探索”，它利用的是 K8s 的​**声明式 API（Declarative API）**​。它在遍历一个标准的、确定性的状态树，而不是在未知的丛林中探险。这就好比查字典，而非破案。这是一种\*\*“伪自主”\*\*。

一旦问题溢出 K8s 层级（例如业务代码逻辑死锁，但 Pod 状态依然是 Running），这些 Agent 瞬间就会失效。

## 四、 最终架构：LLM as a Node，Not the Brain

基于以上推导，我得出了 AIOps 落地的最终结论：

**放弃“全自动驾驶”，拥抱“辅助驾驶”。**

既然 LLM 缺乏排障的“过程记忆”，我们就不能指望它掌控工作流（Workflow）。它不应该是指挥官，而应该是流水线上的特定​**功能节点**​。

**新的范式如下：**

1. **控制权回归代码：** 使用 Python 或工作流引擎（Workflow Engine）固化 SOP。排查流程的主干必须是企业内部沉淀下来的、确定性的专家经验。
2. **LLM 的正确生态位：** 在这个固定的流程中，利用 LLM 的通用泛化能力：
   * **思维链（Chain of Thought）：** 在流程的特定卡点，让 LLM 基于上下文提出假设（“基于这些指标，列出 3 个可能的根因”），再由固定脚本去验证。
   * **非结构化转结构化：** 从杂乱的日志中提取实体。
   * **逻辑反驳（Critic）：** “如果结论是 A，但现象包含 B，这是否矛盾？”

我们不再强求 LLM 像人一样去“操作”系统，而是让 LLM 为确定性的脚本提供“思考力”和“联想力”。

## 结语

通往 AIOps 的道路，不是依靠 Prompt 堆砌出来的“魔法”，而是对数据本质的深刻理解。

承认 LLM 在运维领域的\*\*“训练数据真空”\*\*，是走向成熟的第一步。在某一天大厂们愿意开源其内部的 Incident Post-mortem 数据库之前，**SOP + LLM Node** 才是当下最稳健、最高效的解法。
