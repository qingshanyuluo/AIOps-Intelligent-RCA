# Architecture Evolution Log (架构演进日志)

本文档记录 **AIOps-Intelligent-RCA** 项目从 0 到 1 的核心架构决策、设计权衡（Trade-offs）以及迭代思考过程。

---

## 📅 2025-12-11: Phase 1 - 核心架构确立与“反事实”机制引入

### 1. Context (背景与痛点)

在早期的 POC (概念验证) 阶段，直接使用 LLM 分析告警日志存在两个致命问题：

* **严重的幻觉 (Hallucination):** 模型倾向于顺着日志的字面意思“猜”结论，而不是查证。
* **实时查询导致benchmark困难**无法沉淀案例，全量备份监控数据困难

### 2. Key Decisions (关键决策)

#### A. 引入主动反事实验证 (Active Counterfactual Verification)

* **设计:** 放弃单向的 Chain-of-Thought (CoT)，采用 `Hypothesis` -> `Counterfactual Reasoning` -> `Verification` 的闭环结构。
* **逻辑:** "如果 X 是根因，那么指标 Y 必然异常"。通过这一逻辑强制 Agent 调用工具查证。
* **价值:** 将根因分析从“文本生成任务”转变为“科学实验任务”，大幅降低误报率。

#### B. 确立 Retrieval-as-a-Tool (RaaT) 与数据预处理

* **设计:** 查询聚合后的关键指标日志等上下文数据作为原始数据存储，加工后再使用RaaT替代手动开发tool减少开发工作量
* **优化:** 实施 **Data Pre-collection (数据预存)** 策略。
  * 并不让 Agent 实时去拼写 PromQL（容易出错）。
  * 而是预先聚合关键指标（错误率、P99、饱和度）并存储为结构化上下文，供 Agent 快速检索。
* **价值:** 实现了 Agent 决策层与底层数据设施的解耦，提升了推理速度，并让过程可复现，方便后续prompt和流程优化

---

## 📅 2025-12-11: Phase 2 - 解决“断链”与微服务雪崩难题

### 1. Problem (新发现的问题)

在模拟 **“线程池满” (Thread Pool Exhaustion)** 等级联故障场景时，发现传统的基于 Trace 的分析失效了。

* **现象:** 上游 A 调用 B 超时，Trace 在 A 处中断。
* **盲区:** 真正的根因（下游 C 变慢）因为没有生成完整的 Trace 跨度，导致 Agent 无法顺着 Trace 找到 C。

### 2. Architectural Upgrade (架构升级)

#### A. 提出“全局异常交集”逻辑 (Global Anomaly Intersection)

* **设计:** 从“单兵追踪”升级为“全景侦查”。
  * **Input 1:** 实时维护一份“全局嫌疑人名单” (Global Suspicious List)，包含全公司范围内 P99 或错误率突增的 Top N 应用。
  * **Input 2:** 故障入口服务的下游依赖关系（基于历史数据或轻量级拓扑）。
* **逻辑:**`Root Cause Candidates = Intersection(Global_Anomalies, Downstream_Dependencies)`。
* **价值:** 即使 Trace 断裂，Agent 也能通过“时间相关性”和“拓扑相关性”的交集，精准锁定没有出现在 Trace 里的下游故障节点。

#### B. 存储策略调整 (Sparse Topology)

* **设计:** 为了支持上述逻辑且不引入重型 CMDB，决定只存储 ​**“异常边” (Abnormal Edges)**​。即只记录那些发生过高耗时或错误的调用关系。
* **价值:** 极大降低了数据存储成本，去除了正常链路的噪音。

---

## 📅 2025-12-15: Phase 3 - Retrieval-as-a-Tool 的范式转移：从 SQL 到文件系统隐喻

### 1. Context (背景与痛点)

**在落地** **Retrieval-as-a-Tool (RaaT)** **机制时，需要让 Agent 从预处理好的“灾难现场快照” (JSON 格式) 中提取信息。**
初期尝试引入 **DuckDB** **作为中间层，让 LLM 编写 SQL 进行查询，但遭遇了显著的“ROI”问题：**

* **SQL 生成脆性:** **LLM 在处理简单的** **SELECT \*** **时表现尚可，但在涉及 JSON 字段提取和嵌套查询时，错误率显著升高。**
* **杀鸡用牛刀:** **面对的数据对象已经是经过清洗的、行数有限的 Context JSON，引入数据库引擎显得过于厚重，且并未带来预期的检索精度提升。**

### 2. Key Decisions (关键决策)

#### A. 转向“文件系统隐喻”交互 (File System Metaphor Interface)

* **设计:** **放弃 SQL 接口，重构为模拟 Linux/ZooKeeper 的文件系统交互模式。**
* **工具集:** **精简为 4 个原子工具：**
  
  * **ls**: 探索当前上下文结构（看目录）。
  * **cat**: 读取具体的数据切片（读文件）。
  * **grep**: 跨维度的关键词搜索（如全局搜索 TraceID）。
  * **guide**: 获取当前层级数据的元信息与分析建议。
* **价值:** **利用 LLM 在预训练阶段对 Shell/Linux 环境的深刻直觉（Native Intuition），降低了 Tool Use 的学习成本，比生成的 SQL 更鲁棒。**

#### B. 引入动态导航机制 (The "Guide" Pattern)

* **灵感:** **参考 Anthropic 最新的 Agent Skills 设计理念，强调 Context 与 Data 的分离。**
* **设计:** **实现** **"Guide/README"** **机制。**
  
  * **Agent 进入某个数据目录时，先读取** **guide**。
  * **guide** **仅告知“这里有什么数据”以及“建议怎么用”，而不直接 dump 数据。**
* ​**价值:** **实现了上下文的** **按需加载 (Lazy Loading)**。Agent 不再会被海量 Context 淹没，而是像人类专家一样，先看目录索引，再按需调取细节。这极大节省了 Token 开销，并减少了无关信息对推理的干扰。

## 📅 2025-12-16: Phase 3.5 - 观测层升级：从“原始数据搬运”到“语义化渲染”

### 1. Context (背景与痛点)

**在实现了基础的** **cat** **工具后，我们发现 Agent 在处理时序数据（Metrics）时面临两难：**

* **数值盲区 (Numerical Blindness):** **LLM 难以仅凭一串浮点数（如** **[0.1, 0.2, ... 99.0]**）敏锐地感知趋势变化，且容易算错统计值。
* **Token 效率低:** **简单的 Top-N 时序数据平铺会消耗大量 Token，且分散了 Agent 的注意力。**
* **日志、告警、变更等原始json信息密度低，含有大量无用tag，消耗大量 Token，且分散了 Agent 的注意力**

### 2. Key Decisions (关键决策)

#### A. 确立 "Rich-Observation" 交互范式

* **设计:** **拒绝简单的** **file.read()**。确立 **Tool Should Be Smart** **的原则，工具不仅负责取数，更负责\*\*“渲染 (Rendering)”\*\*。**
* **实现:** **在** **cat** **工具中引入** **ASCII Sparklines (字符趋势图)** **技术。**
  
  * **将** **[10, 12, ..., 98]** **渲染为** **▂▃▄▆██**。
  * **价值:** **利用 LLM 强大的视觉/形状语义理解能力，替代脆弱的数值推理能力，实现“一眼看穿”故障趋势。**

#### B. 采用“格式化器注册”模式 (Formatter Registry Pattern)

* **问题:** **随着 Log、Trace、Metrics 多种数据类型的引入，单一的** **cat** **函数逻辑变得臃肿。**
* ​**架构:** **借鉴** **Strategy Pattern (策略模式)**，构建了可扩展的插件架构：
  
  ​* **SmartCatTool** **作为入口，维护一个** **Formatter** **列表。**
  
  * **TimeSeriesFormatter**: 负责计算 P99、绘制 Sparklines、异常点高亮。
  * **LogFormatter**: 负责日志降噪、错误堆栈折叠。
  * **Event/AlertFormatter**: 负责精简json，形成节省token，可读性良好的markdown格式的事件告警描述
* ​**价值:** **实现了** **Open/Closed Principle (开闭原则)**。未来新增数据类型（如拓扑图文本化），无需修改核心逻辑，只需注册新的 Formatter。​

### 3. Impact (影响)

* **精度提升:** **Agent 对“瞬间脉冲 (Spike)”的识别率显著提升，幻觉误报率大幅下降。**
* **成本优化:** **通过“智能折叠”非关键数据，单次诊断的 Token 消耗降低了约 40%。**


## 📅 2025-12-18: Phase 4 - 自动化执行固定流程分析，产出高质量单应用报告

*  **问题:** **利用llmagent自主查询执行固定化的分析流程既耗时又浪费token，大模型注意力稀释**
*  ​**架构:** 流程化，固定化，统计学方法整理指标

详情：[指标分析算法](https://github.com/qingshanyuluo/AIOps-Intelligent-RCA/blob/main/doc/design/timeseries-analysis.md)

* ​**价值:** **指标分析几分钟=>毫秒级**

## 📅 2025-12-18: Phase 5 - 交互协议重构：强制“思考后行动”

### 1. Problem (痛点)

模型在 Function Calling 模式下有严重的“扳机指 (Trigger-Happy)”现象：为了追求响应速度，往往跳过推理直接抛出 JSON。这导致 Agent 陷入“盲目试错”循环，且缺乏可追溯的思维链路，调试困难。

### 2. Key Decisions (关键决策)

#### A. 实施“工具内嵌思考” (In-Tool Reasoning Injection)

* **设计:** 修改所有工具的 Schema，强制加入一个必填参数 ​**rationale ​(调用理由)**​。
* **机制:** 利用 JSON 线性生成的特性，迫使模型在生成具体业务参数（如 metric\_name）之前，必须先写出 rationale。
* **价值:** 在协议层面（Schema Level）物理强制了 CoT（思维链），比纯 Prompt 约束更稳健，实现了“想清楚再动手”。

#### B. 放弃“两步法” (Rejection of Two-Step Generation)

* **权衡:** 虽然“先生成思考文本，再调用工具”的准确率最高，但会带来双倍延迟。
* **结论:** **Schema 注入方案** 在单次 API 调用中同时完成了“思考”与“执行”，在保持低延迟的同时，解决了 90% 的盲目调用问题。

### 3. Impact (影响)

* **自愈能力:** 模型在填写 rationale 过程中经常出现“自我纠正”行为，无效工具调用大幅减少。
* **可观测性:** 现在的 Agent 日志天然包含了“为什么做”的逻辑轨迹，极大方便了后续优化。


## 📅 2025-12-19: Phase 6 - 混合智能架构：从“经验直觉”到“统计科学” (Hybrid Intelligence)

### 1. Context (背景与痛点)

**随着监控规模的扩大，我们遭遇了 AIOps 领域的经典难题——“基础设施共模故障 (Common Mode Failure)”。**

* **场景:** 当底层网络专线发生物理抖动或丢包时，上层数十个无依赖关系的应用会同时抛出超时告警。
* **Agent 的困境:** LLM 缺乏全局物理视角，倾向于将每个应用的报错孤立看待（认为是代码 Bug），瞬间并发数十个无意义的分析任务。这导致 Token 消耗爆炸，且最终结论往往是错误的。

### 2. Key Decisions (关键决策)

#### A. 引入“确定性模式识别”拦截层 (Deterministic Pattern Interceptor)

* **决策:** **AI 不是万能的，规则 + AI 才是。** 在 Agent 介入前，引入基于专家经验的 **System 1 (快思考)** 拦截层。
* **初版算法 (Based on Expert Intuition):**
  我们将资深 SRE 的经验——“大面积报错 + 阶梯状分布 = 网络抖动”——转化为启发式规则：
  
  * **Scope:** 报错应用数 > Threshold (e.g., 20)。
  * **Shape:** 错误数排序后呈\*\*“平原/阶梯状”\*\*分布，而非个别应用的“脉冲状”。
  * **Variance:** Top N 应用的报错数量差距​**不超过 3 倍**​。
* **逻辑:** 只有底层物理设施（路面）出问题时，跑在上面的车（应用）才会无论型号大小，都出现相似幅度的颠簸。

#### B. 算法迭代：归一化与时空相关性修正 (The "Normalized Co-Occurrence" Upgrade)

* **问题 (The QPS Trap):** 初版算法基于“绝对报错数”，忽略了应用间的 QPS 差异。高并发应用的报错数自然更高，容易掩盖低并发应用的异常，导致漏判。
* **架构升级:** 为了提升算法在异构环境下的鲁棒性，我们将“老师傅经验”升级为严格的​**统计学检测 (Statistical Detection)**​：
  
  * **Metric Normalization (归一化):** 将比较维度从 ErrorCount 升级为 ErrorRate。
    
    * 逻辑: 无论 QPS 差多少倍，在物理丢包面前，受难程度（错误率）应保持一致（Homogeneity）。
  * **Temporal Alignment (时序对齐):** 引入 ​**Pearson Correlation (皮尔逊相关系数)**​。
    
    * 逻辑: 计算 Top N 应用错误曲线的相关性。若
      
      ```rendered
      R>0.85
      ```
      
      ，证明它们在同频共振，进一步排除巧合噪音。
  * **Topological Isolation (拓扑离散性):**
    
    * 逻辑: 验证这些报错应用在调用链图谱上是​**离散的点 (Isolated Nodes)**​，而非连通子图，彻底区分“网络震荡”与“服务雪崩”。

#### C. 短路机制 (Short-Circuit Mechanism)

* **设计:** 一旦拦截层命中上述特征：
  
  * **立即终止** 后续所有 LLM 的 Root Cause Analysis 任务。
  * **直接输出** 确定性结论：“检测到共模故障（如网络专线抖动），涉及应用 N 个，请检查基础设施。”

### 3. Impact (影响)

* **降噪比:** 在网络抖动演练中，无效的 LLM 调用减少了 ​**100%**​，避免了“一本正经胡说八道”的幻觉。
* **成本骤降:** 此类大规模故障的诊断成本从无意义的llm推理降低到了**0 (纯规则计算)**。
* **响应速度:** 诊断时间从分钟级（Agent 逐个分析）缩短至 ​**毫秒级**​。
* **架构意义:** 成功实现了 ​**Hybrid Intelligence (混合智能)**​，证明了在 T0 级系统中，优质的规则代码可以成为 AI 的“脊柱”。

## 📅 2025-12-23: Phase 7 - 根因分析的“入口纠偏”：基于 Trace 权重的隐性瓶颈挖掘 (Weighted Bottleneck Discovery)

### 1. Context (背景与痛点)

在拥有了全量的报错 RPC 链路数据后，我们发现单纯依赖“最深报错节点”作为分析入口往往是错误的。
**核心矛盾：** 报错节点通常只是“受害者” (Victim)，真正的“始作俑者” (Culprit) 往往隐藏在链路的某处，**表现为高耗时但未报错**，导致 Agent 分析错方向。

常见的**“入口误判”**场景：

- **误判为下游错:** 实际上是 **上游** 自身逻辑太重（或 N+1/重试）耗尽了时间，导致下游随机超时。
- **误判为上游错:** 实际上是 **下游** 某服务默默变慢（但没挂），拖累了整条链路。
- **误判为应用错:** 实际上是 **中间链路** (网络/序列化) 偷走了时间。

我们需要一套机制，在把数据喂给 LLM 之前，先通过算法**重新衡量全局的探索入口**。

### 2. Key Decisions (关键决策)

#### A. 算法目标重构：寻找“最大时间掠夺者” (The Time Thief)

- **决策:** 算法不再是为了检测特定 Bug（如 N+1），而是为了**竞价排名**。
- **逻辑:** 在一条报错 Trace 中，谁“掠夺”的时间权重最大，谁就是真正的**分析入口**。
- **权重公式 ($W$):** 将 时间、次数、网络差值 统一折算为“阻力值”。
  $$W = (AvgLat \times Count) + \Delta(Network) + Penalty(Error)$$
  *(注：即便节点没报错，只要前两项乘积够大，它就能胜出，成为入口)*

#### B. 分析入口的动态纠偏策略 (Dynamic Entry Rectification)

利用上述权重，对报错链路进行**“折叠 (Folding) 与 归因”**，将分析焦点从“报错点”强制转移：

1. **纠偏至上游 (Upstream-Centric):**
  
  - *现象:* 某条边 (Edge) 的 $W$ 极高，且主要由 `Count` (次数) 或 `Gap` (网络) 贡献。
  - *判定:* 下游是无辜的。**分析入口强制回调至上游节点**。
  - *场景覆盖:* N+1、重试风暴、序列化阻塞。
2. **纠偏至下游 (Downstream-Centric):**
  
  - *现象:* 某下游节点的 $W$ 极高，主要由 `Duration` (耗时) 贡献，且 `Count` 正常。
  - *判定:* 上游的超时是被害的。**分析入口穿透至该隐性慢节点**。
  - *场景覆盖:* 下游服务处理慢（但未熔断/未报错）。
3. **锁定自身 (Self-Centric):**
  
  - *现象:* 所有出度的 $W$ 都很低，但节点的 `Self-Time` (总耗时 - 下游权重) 极高。
  - *判定:* **分析入口锁定在当前节点内部**。
  - *场景覆盖:* 自身 CPU/GC/锁竞争。

#### C. 数据流设计：Trace 聚合作为 LLM 的“前置雷达”

- **输入:** 批量含有错误的 Trace 数据。
- **处理:** 不依赖 Prometheus 查询，纯内存计算 Trace 树的聚合权重。
- **输出:** 向 LLM 提供明确的 **“Target Service”**。
  - *Before:* "A 报错了，请分析 A。" (LLM 容易晕)
  - *After:* "虽然 A 报错了，但算法识别到 B 服务在链路中权重占比 80% (隐性慢)。请忽略 A，重点分析 B 的性能指标。"

### 3. Impact (影响)

- **彻底解决“隐性死角”:** 无论是“上游作死疯狂调下游”还是“下游默默变慢”，算法都能精准识别出**谁才是那个消耗时间的大户**，不再被“Error 标签”带偏。
- **Agent 准确率跃升:** 确保 LLM 第一时间拿到的就是**真正的案发现场**，而不是受害现场。
- **低成本全覆盖:** 仅利用现有的错误链路数据，无需额外的监控存储成本，即可覆盖 N+1、网络抖动、重试风暴等多种复杂场景。

## 🔮 Future Roadmap (未来规划)

* **[Planned]** 引入multagent架构、扩展能力指至基础层、代码层...
* **[Planned]** 部分接入私有化微调模型、降低成本
